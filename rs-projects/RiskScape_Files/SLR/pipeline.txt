# parameters:
# $regions: regional aggregation-layer for reporting results
# $cc_projection: CSV containing the IPCC SSP and associated SLR projections
# $cc_scenarios: picks an SSP climate change scenario out of the $cc_projection to use
# $cc_percentiles: picks an uncertainty percentile associated with the SSP climate change scenario
# parameters:
# $hazard_maps: A CSV that points to other hazard .tif files to load
# $buildings: building exposure-layer, as file/bookmark
# $population: population density raster data
# $crops: crops exposure-layer (points), as file/bookmark
# $infrastructure_points: exposure-layer containing infrastructure that can be represented by point geometry
# $infrastructure_polygons: exposure-layer containing infrastructure that can be represented by polygon geometry
# $infrastructure_lines: exposure-layer containing infrastructure that can be represented by line geometry
# $reproject_to: A CRS suitable for representing the country's geometry in metre units
# $cut_distance: size in metres to segment polygons/lines by
#
# Input hazard data
#
# this is probabilistic data, so the input is a CSV where each row is a hazard .tif
input($hazard_maps, name: 'event') as hazard_input
 ->
select({ { event.*, bookmark(id: $shared_lib + event.file, options: {}, type: 'coverage(floating)') as coverage } as event })
 ->
group(by: { event.group as event_group },
     select: {
        *,
        to_lookup_table(key: { event.SLR, event.Return_Period }, value: { event.coverage }, options: { unique: true }) as coverage_table
   })
 ->
join_exposures_and_events.rhs

#
# Input: SLR projections
#
input(bookmark('IPCC_projections', { location: $cc_projection }))
 ->
filter(switch(scenario, default: false, cases: [{ in: $cc_scenarios, return: true }]) and
       switch(percentile, default: false, cases: [{ in: $cc_percentiles, return: true }]))
 ->
# the projections come in as a list, due to the shape of the input data
unnest(projections)
 ->
select({ scenario as Scenario, percentile as Percentile, projections.* }) as SLR_projections
 ->
group(select: {
        to_lookup_table(key: { Scenario, Percentile, Year }, value: SLR, options: { unique: true }) as SLR_table
      })
 ->
select({ *, $cc_scenarios as Scenario, $cc_percentiles as Percentile })
 ->
unnest(Percentile)
 ->
unnest(Scenario)
 ->
select({
          *,
          # this maps a given year to an interpolated SLR
          create_continuous(
              # the assumption here is we will have projected SLR data-points for all these hard-coded years
              # in the cc_projection CSV file. We can then use them to interpolate to get the requested $years
              [2020, 2030, 2040, 2050, 2060, 2070, 2080, 2090, 2100, 2110, 2120, 2130, 2140, 2150],
              x -> if_null(
                           lookup(SLR_table, { Scenario, Percentile, int(x) as Year }),
                           0.0)
          ) as SLR_curve,
          $years as Year,
          $return_periods as Return_Period
})
 ->
unnest(Year) as unnest_years
 -> 
# the user-specified year might not exactly match the cc_projection data.
# If that's the case we use interpolation to work out the SLR (rounded to nearest cm)
select({ *, apply_continuous(SLR_curve, Year) as SLR }) as interpolate_SLR
 ->
# sanity-check we don't exceed what we have SLR data for
filter(is_not_null(SLR) and SLR >= 0.0 and SLR <= 2.0) as filter_no_SLR
 ->
# SLR is guaranteed to be non-null at this point, but this keeps RS type-checking happy
select({ *, if_null(SLR, 0.0) as SLR })
 ->
join_SSPs.rhs

#
# Input Exposures
#
input($population, name: 'exposure') as input_population
->
select({ *, 'Population' as InputType })
->
union() as all_exposures

input($buildings, name: 'exposure') as input_buildings
->
select({ *, 'Building' as InputType })
->
all_exposures

input($evac_centres, name: 'exposure') as input_evac_centres
->
select({ *, 'Evacuation Centre' as InputType })
->
all_exposures

input($health_facilities, name: 'exposure') as input_health_facilities
->
select({ *, 'Health Facility' as InputType })
->
all_exposures

input($schools, name: 'exposure') as input_schools
->
select({ *, 'School' as InputType })
->
all_exposures

input($roads, name: 'exposure') as input_roads
 ->
select({ *,  'Road' as InputType })
 ->
all_exposures

input($crops, name: 'exposure') as input_crops
 ->
select({ *,  'Crop' as InputType })
 ->
all_exposures

input($airports, name: 'exposure') as input_airports
 ->
select({ *,  'Airport' as InputType, true as infrastructure })
 ->
all_exposures

input($ports, name: 'exposure') as input_ports
 ->
select({ *,  'Port' as InputType, true as infrastructure })
 ->
all_exposures

input($power, name: 'exposure') as input_power
 ->
select({ *,  'Power' as InputType, true as infrastructure })
 ->
all_exposures

input($telecommunications, name: 'exposure') as input_telecom
 ->
select({ *,  'Telecommunication' as InputType, true as infrastructure })
 ->
all_exposures

input($water, name: 'exposure') as input_water
 ->
select({ *,  'Water' as InputType, true as infrastructure })
 ->
all_exposures

input($bridges, name: 'exposure') as input_bridges
 ->
select({ *,  'Bridge' as InputType, true as infrastructure })
 ->
all_exposures

#
# Geoprocessing & Sampling
#
all_exposures
 ->
# we have tonnes of hazard maps organized into localities, e.g. island groups.
# Find the group of .tif files that relate to this exposure, so we only have a single
# hazard/impact per exposure. In other words, this exposure should only
# fall within the bounds of one group of hazard maps and we can ignore the other hazard maps
select({ *, sample_one(exposure.geom, to_coverage(bookmark($hazard_index)), buffer-distance: 5000.0) as hazard_index }) as sample_hazard_group
 ->
# can't really do anything with exposures that fall outside the hazard maps
filter(is_not_null(hazard_index)) as filter_outside_hazard_coverage
 ->
# strip nullability (the previous step already removes any possibility it is null)
select({ *, if_null(hazard_index.group, '') as group })
 ->
# reproject to a metric CRS to make measure/segment operations quicker for polygons/lines.
select({ *, reproject(exposure.geom, $reproject_to) as geom }) as reproject
 ->
select({ *, switch(InputType, default: false, cases: [{ in: $can_segment, return: true }]) as can_segment })
 ->
# segment any lines/polygons by a fixed distance (not by a grid).
# If any part of segment is exposed, then the whole segment is exposed.
select({ *, measure(geom) as orig_size, segment(geom, $cut_distance) as segmented_geom }) as segment
 ->
# NB: don't segment buildings or other discrete assets
select({ *, if_then_else(can_segment, segmented_geom, [ geom ]) as geom })
 ->
unnest(geom)
 ->
select({ *, measure(geom) as size }) as measure
 ->
# NB: use scale_factor=1.0 for points, so we don't end up with NaN
select({ *, if_then_else(can_segment and size > 0, size / orig_size, 1.0) as scale_factor })
 ->
select({
         *,
         merge(exposure, {
                           geom,
                           exposure.Value * scale_factor as Value,
                           InputType as Asset,
                           if_null(infrastructure, false) as Infrastructure,
                           round(size, 2) as Size
                         }) as exposure
       }) as scale_exposures
 ->
# join up each exposure to the group of hazard maps that it falls within
join(hazard_index.group = event_group) as join_exposures_and_events
 ->
# we have a tonne of data to crunch, so we want to get rid of what we can as quick as possible.
# If an exposure is unexposed to the 2m ARI250 event, then it shouldn't be exposed
# to any other events. We can ignore it without sampling the other 146 hazard maps
select({ *, lookup(coverage_table, { SLR: 2.0, Return_Period: 250 }) as worst_case_coverage }) as check_worst_case
 ->
filter(is_not_null(sample_one(geom, worst_case_coverage))) as filter_unexposed
 ->
select({
         *,
         sample_one(exposure, to_coverage(bookmark($map_summary)), $region_buffer_km * 1000.0) as map_region,
         sample_one(exposure, to_coverage(bookmark($table_summary)), $region_buffer_km * 1000.0) as table_region,
       }) as exposures_and_regions

#
# Losses for projected SLR
#
exposures_and_regions
 ->
# build a RP+SLR=>loss curve for each affected exposure
select({
         *,
         # this maps a given SLR to an expected loss for a given return period
         create_continuous_2d(
             x-values: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0],
             y-values: [1, 5, 10, 25, 50, 100, 250 ], 
             apply-to: (x, y) ->
                         max(
                              map(
                                   sample(exposure, lookup(coverage_table, { x as SLR, int(y) as Return_Period })),
                                   h -> h.sampled
                              )
                         )
         ) as depth_curve
       })
 ->
 select({
     *,
     create_continuous_2d(
       x-values: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0],
       y-values: [1, 5, 10, 25, 50, 100, 250 ], 
       apply-to: (x, y) -> map(apply_continuous(depth_curve, x, y), depth -> {
           Depth: if_null(depth, 0.0),
           Loss: if_null(Flood_Loss(exposure, depth).Direct_Loss, 0.0)
      })
    ) as loss_curve,
    { 0.0 as Depth, 0.0 as Loss } as Zero_Loss
   }) as per_exposure_loss_curve
 ->
# skip this exposure if it's not exposed to the max ARI for this SLR
join(on: true) as join_SSPs
 ->
select({ *, apply_continuous(loss_curve, SLR, 250) as ARI250_Impact }) as ARI250_impact
 ->
filter(ARI250_Impact.Depth > $min_depth) as filter_exposed_by_year
 ->
unnest(Return_Period) as unnest_ARIs
 -> 
# use interpolation to work out the impact/loss based on the projected SLR
select({ *, apply_continuous(loss_curve, SLR, Return_Period) as Impact }) as interpolate_loss
 ->
select({
         *,
         # the asset might not have been exposed at this particular RP
         # Use a min_depth here, so we don't overcount when interpolating between a zero depth
         # and a negligibly small depth, e.g. interpolating between 0.0 and 0.02 might produce
         # an interpolated depth of 0.002, which we don't really care about.
         if_then_else(Impact.Depth >= $min_depth, Impact, Zero_Loss) as Impact,
         1 - exp(0 - (1/float(Return_Period))) as Exceedance_Probability,
         round(SLR, 3) as SLR
      })
 ->
select({
     *,
     Impact.Depth > 0 as Exposed
})
 ->
select({
         *,
         Impact.Loss as Total_Loss,
         if_then_else(exposure.Infrastructure, 'Infrastructure', exposure.Asset) as Asset_Type,
         if_then_else(exposure.Asset = 'Building', Impact.Loss, 0.0) as Building_Loss,
         if_then_else(exposure.Asset = 'Road', Impact.Loss, 0.0) as Road_Loss,
         if_then_else(exposure.Infrastructure, Impact.Loss, 0.0) as Infrastructure_Loss,
         if_then_else(exposure.Asset = 'Crop', Impact.Loss, 0.0) as Crop_Loss,
         if_then_else(exposure.Asset = 'Building' and Exposed, exposure.Value, 0.0) as Exposed_Building_Value,
         if_then_else(exposure.Asset = 'Road' and Exposed, exposure.Value, 0.0) as Exposed_Road_Value,
         if_then_else(exposure.Infrastructure and Exposed, exposure.Value, 0.0) as Exposed_Infrastructure_Value,
         if_then_else(exposure.Asset = 'Crop' and Exposed, exposure.Value, 0.0) as Exposed_Crop_Value,
         if_then_else(exposure.Asset = 'Population' and Exposed, exposure.Value, 0.0) as Exposed_Population,
         if_then_else(exposure.Asset != 'Population' and Exposed, exposure.Value, 0.0) as Exposed_Total_Value,
         if_then_else(exposure.Asset = 'Road' and Exposed, exposure.Size / 1000, 0.0) as Exposed_Road_km,
         if_then_else(exposure.Asset = 'Population' and $household_pop, true, null_of('boolean')) as Household
      }) as projected_SLR_event_impact_table

#
# Reporting: Regional impact
#
projected_SLR_event_impact_table
 ->
# we create a dummy 'National' region here and duplicate each row of data, so that
# we can produce the regional and national results in a single 'group' step
# (which avoids duplicating complicated pipeline steps)
select({
        *,
        [{ null_of('text') as ID, null_of('text') as Region, 'National' as Level },
         { table_region.ID, table_region.Region, 'Admin1' as Level },
         { map_region.ID, map_region.Region, 'Admin2' as Level }] as Region,
         round(Exceedance_Probability, 4) as Exceedance_Probability
      })
 ->
unnest(Region) as unnest_regions
 ->
# at the admin2/map-level we only have one row of data per region, so we use the SSP 2-4.5 p50 AALs.
# We filter out the admin2-level results that we're not interested in here as it saves a lot of
# memory-intensive aggregation (it saves ~226K group-by buckets)
filter(Region.Level != 'Admin2' or (Scenario = 'ssp245 (medium confidence)' and Percentile = 50)) as filter_admin2_regions
 ->
group(by: { Region, Scenario, Percentile, Year, SLR, Return_Period, Exceedance_Probability },
      select: {
          *,
          count(Asset_Type = 'Building' and Exposed) as Exposed_Buildings,
          count(Building_Loss > 0) as Damaged_Buildings,
          count(Asset_Type = 'Building' and exposure.UseType = 'Residential' and Exposed) as Exposed_Residential_Buildings,
          count(Asset_Type = 'Building' and exposure.UseType != 'Residential' and Exposed) as Exposed_Nonresidential_Buildings,
  
          round(sum(Building_Loss)) as Building_Loss,
          round(sum(if_then_else(exposure.UseType = 'Residential', Building_Loss, 0.0))) as Residential_Building_Loss,
          round(sum(if_then_else(exposure.UseType != 'Residential', Building_Loss, 0.0))) as Nonresidential_Building_Loss,
  
          round(sum(Exposed_Building_Value)) as Exposed_Building_Value,
  
          # Population
          round(sum(Exposed_Population)) as Exposed_Population,
          count(Household and Exposed) as Exposed_Households,
  
          # Roads
          round(sum(Exposed_Road_km), 2) as Exposed_Road_km,
          round(sum(Road_Loss)) as Road_Loss,
          round(sum(Exposed_Road_Value)) as Exposed_Road_Value,
  
          # infrastructure
          round(sum(Infrastructure_Loss)) as Infrastructure_Loss,
          round(sum(Exposed_Infrastructure_Value)) as Exposed_Infrastructure_Value,
          count(Asset_Type = 'Infrastructure' and Exposed) as Exposed_Infrastructure,
  
          # crops
          round(sum(Crop_Loss)) as Crop_Loss,
          round(sum(Exposed_Crop_Value)) as Total_Crop_Value,
  
          # exposed schools, health, etc
          count(Asset_Type = 'Evacuation Centre' and Exposed) as Exposed_Evacuation_Centres,
          count(Asset_Type = 'Health Exposedity' and Exposed) as Exposed_Health_Facilties,
          count(Asset_Type = 'School' and Exposed) as Exposed_Schools,
  
          round(sum(Exposed_Total_Value)) as Total_Exposed_Value,
          round(sum(Total_Loss)) as Total_Loss,
  
          bucket_range(pick: Impact.Depth,
                range: $flood_depths,
                select: {
                count(Asset_Type = 'Building' and Exposed) as Buildings,
                round(if_null(
                      sum(Exposed_Population),
                      0.0)) as Population,
          }) as Depth,
      }) as aggregate_impact
 ->
filter(Region.Level = 'Admin1') as filter_admin1_regions
 ->
select({ Region.ID as Region_ID, *, Region.Region as Region }) as regional_event_impact
 ->
sort([ Region, Scenario, Percentile, Year, SLR, Return_Period])
 ->
save('regional-impact', format: 'csv')

#
# Reporting the results - total/national event-loss table
#
aggregate_impact
 ->
filter(Region.Level = 'National') as national_event_impact
 ->
select({ *, 'National' as Region })
 ->
sort([Scenario, Percentile, Year, SLR, Return_Period])
 ->
save('event-impact', format: 'csv')

#
# National AAL
#
aggregate_impact
 ->
group(by: { Region, Scenario, Percentile, Year, SLR },
      select: {
            *,
            round(aal_trapz(loss: Total_Loss, ep: Exceedance_Probability), 2) as Total_AAL,
            round(aal_trapz(loss: Building_Loss, ep: Exceedance_Probability), 2) as Building_AAL,
            round(aal_trapz(loss: Crop_Loss, ep: Exceedance_Probability), 2) as Crops_AAL,
            round(aal_trapz(loss: Road_Loss, ep: Exceedance_Probability), 2) as Road_AAL,
            round(aal_trapz(loss: Infrastructure_Loss, ep: Exceedance_Probability), 2) as Infrastructure_AAL,
            round(aal_trapz(loss: Exposed_Population, ep: Exceedance_Probability), 2) as Average_Annual_Population_Exposed
      }) as aggregate_aal
 ->
filter(Region.Level = 'National') as filter_national_results
 ->
select({ *, 'National' as Region })          
 ->
sort([Scenario, Percentile, Year, SLR])
 ->
save('average-loss', format: 'csv')

#
# Regional AAL (by year)
#
aggregate_aal
 ->
filter(Region.Level = 'Admin1') as filter_admin1_AAL
 ->
select({ Region.ID as Region_ID, *, Region.Region as Region })
 ->
sort([Region, Scenario, Percentile, Year, SLR])
 ->
save('regional-average-loss', format: 'csv')

#
# Regional summary
#
aggregate_aal
 ->
filter(Region.Level = 'Admin2') as filter_admin2_AAL
 ->
# NB: for admin2, we've already filtered out just the SSP2-4.5 p50 scenario earlier
# in the filter_admin2_regions step (to save ourself unnecessary aggregation work)
group(by: { Region, Scenario, Percentile },
      select: {
            *,
            bucket(by: Year,
                   pick: b -> b = Year,
                   select: {
                        # we should only end up with one result per bucket
                        max(Year) as Year,
                        max(Total_AAL) as Total_AAL,
                        max(Building_AAL) as Building_AAL,
                        max(Crops_AAL) as Crops_AAL,
                        max(Road_AAL) as Road_AAL,
                        max(Infrastructure_AAL) as Infrastructure_AAL,
                        max(Average_Annual_Population_Exposed) as Average_Annual_Population_Exposed
                    },
                    buckets: {
                        First: min($years), Last: max($years)
                    }) as Bin
      }) as regional_summary
 ->
# add back in the map geometry (we had to drop it earlier to keep RS types happy)
join(on: map_region.Region = Region.Region and map_region.ID = Region.ID) as join_map_region
 ->
select({ map_region.*, Scenario, Percentile, Bin.First, Bin.Last, (Bin.Last - Bin.First) as Change })
 ->
save('regional-summary', format: 'geojson')

input($map_summary, name: 'map_region') as input_admin2_regions
 ->
join_map_region.rhs

per_exposure_loss_curve
 ->
select({ *, 100 as Return_Period })
 ->
# use interpolation to work out the impact/loss based on the projected SLR
select({ *,
         apply_continuous(loss_curve, 0.0, Return_Period) as Impact_0cm,
         apply_continuous(loss_curve, 0.5, Return_Period) as Impact_50cm,
         apply_continuous(loss_curve, 1.0, Return_Period) as Impact_100cm,
         apply_continuous(loss_curve, 1.5, Return_Period) as Impact_150cm
       })
 ->       
select({
         exposure,
         map_region,
         if_then_else(Impact_0cm.Depth >= $min_depth, Impact_0cm, Zero_Loss) as Impact_0cm,
         if_then_else(Impact_50cm.Depth >= $min_depth, Impact_50cm, Zero_Loss) as Impact_50cm,
         if_then_else(Impact_100cm.Depth >= $min_depth, Impact_100cm, Zero_Loss) as Impact_100cm,
         if_then_else(Impact_150cm.Depth >= $min_depth, Impact_150cm, Zero_Loss) as Impact_150cm
       })
 ->
# only include exposed assets in the results
filter(Impact_150cm.Depth > 0) as fixed_SLR_impacts
 ->
filter(exposure.Asset = 'Building') as filter_buildings
 ->
select({
         exposure.*,
         round(Impact_0cm.Depth, 3) as SLR_0cm_ARI100_Depth, Impact_0cm.Loss as SLR_0cm_ARI100_Loss,
         round(Impact_50cm.Depth, 3) as SLR_50cm_ARI100_Depth, Impact_50cm.Loss as SLR_50cm_ARI100_Loss,
         round(Impact_100cm.Depth, 3) as SLR_100cm_ARI100_Depth, Impact_100cm.Loss as SLR_100cm_ARI100_Loss,
         round(Impact_150cm.Depth, 3) as SLR_150cm_ARI100_Depth, Impact_150cm.Loss as SLR_150cm_ARI100_Loss,
       })
 ->
select({ *, reproject(geom, 'EPSG:4326') as geom, str(Infrastructure) as Infrastructure })
 ->
save('buildings-impact', format: 'geopackage')

fixed_SLR_impacts
 ->
filter(exposure.Asset = 'Road') as filter_roads
 ->
select({
         exposure.*,
         round(Impact_0cm.Depth, 3) as SLR_0cm_ARI100_Depth, Impact_0cm.Loss as SLR_0cm_ARI100_Loss,
         round(Impact_50cm.Depth, 3) as SLR_50cm_ARI100_Depth, Impact_50cm.Loss as SLR_50cm_ARI100_Loss,
         round(Impact_100cm.Depth, 3) as SLR_100cm_ARI100_Depth, Impact_100cm.Loss as SLR_100cm_ARI100_Loss,
         round(Impact_150cm.Depth, 3) as SLR_150cm_ARI100_Depth, Impact_150cm.Loss as SLR_150cm_ARI100_Loss,
       })
 ->
select({ *, reproject(geom, 'EPSG:4326') as geom, str(Infrastructure) as Infrastructure })
 ->
save('road-impact', format: 'geopackage')

fixed_SLR_impacts
 ->
select({
         *,
         Impact_0cm.Depth > 0 as Exposed_0cm,
         Impact_50cm.Depth > 0 as Exposed_50cm,
         Impact_100cm.Depth > 0 as Exposed_100cm,
         Impact_150cm.Depth > 0 as Exposed_150cm,
         if_then_else(exposure.Infrastructure, 'Infrastructure', exposure.Asset) as Asset_Type,
       })
 ->
group(by: map_region,
      select: {
            map_region.*,
            bucket(by: Asset_Type,
                   pick: b -> b = Asset_Type or (b = 'Total' and Asset_Type != 'Population'),
                   select: {
                        round(if_null(sum(Impact_0cm.Loss), 0.0)) as Loss,
                        count(Exposed_0cm) as Exposed_Count,
                        round(if_null(sum(if_then_else(Exposed_0cm, exposure.Value, 0.0)), 0.0)) as Exposed_Value
                   },
                   buckets: {
                        Buildings: 'Building', Crops: 'Crop', Roads: 'Road', Infrastructure: 'Infrastructure',
                        Population: 'Population', Total: 'Total'
                   }) as SLR_0cm_ARI100,            
            bucket(by: Asset_Type,
                    pick: b -> b = Asset_Type or (b = 'Total' and Asset_Type != 'Population'),
                    select: {
                        round(if_null(sum(Impact_50cm.Loss), 0.0)) as Loss,
                        count(Exposed_50cm) as Exposed_Count,
                        round(if_null(sum(if_then_else(Exposed_50cm, exposure.Value, 0.0)), 0.0)) as Exposed_Value
                   },
                   buckets: {
                        Buildings: 'Building', Crops: 'Crop', Roads: 'Road', Infrastructure: 'Infrastructure',
                        Population: 'Population', Total: 'Total'
                   }) as SLR_50cm_ARI100,
            bucket(by: Asset_Type,
                    pick: b -> b = Asset_Type or (b = 'Total' and Asset_Type != 'Population'),
                    select: {
                        round(if_null(sum(Impact_100cm.Loss), 0.0)) as Loss,
                        count(Exposed_100cm) as Exposed_Count,
                        round(if_null(sum(if_then_else(Exposed_100cm, exposure.Value, 0.0)), 0.0)) as Exposed_Value
                   },
                   buckets: {
                        Buildings: 'Building', Crops: 'Crop', Roads: 'Road', Infrastructure: 'Infrastructure',
                        Population: 'Population', Total: 'Total'
                   }) as SLR_100cm_ARI100,
            bucket(by: Asset_Type,
                    pick: b -> b = Asset_Type or (b = 'Total' and Asset_Type != 'Population'),
                    select: {
                        round(if_null(sum(Impact_150cm.Loss), 0.0)) as Loss,
                        count(Exposed_150cm) as Exposed_Count,
                        round(if_null(sum(if_then_else(Exposed_150cm, exposure.Value, 0.0)), 0.0)) as Exposed_Value
                   },
                   buckets: {
                        Buildings: 'Building', Crops: 'Crop', Roads: 'Road', Infrastructure: 'Infrastructure',
                        Population: 'Population', Total: 'Total'
                   }) as SLR_150cm_ARI100
    }) as regional_impact_ARI100
 ->
save('regional-impact-ARI100', format: 'geojson') 

# map the 50cm/100cm/150cm increments back to the SSP/year that they will occur in (for context)
SLR_projections
 ->
group(by: { Scenario, Percentile },
      select: {
        *,
        to_lookup_table(key: Year, value: SLR, options: { unique: true }) as SLR_table
      })
 ->
select({
        *,
        create_continuous(
            [2020, 2030, 2040, 2050, 2060, 2070, 2080, 2090, 2100, 2110, 2120, 2130, 2140, 2150],
            x -> lookup(SLR_table, int(x))
        ) as SLR_curve
      })
 ->
select({
         *,
         map(range(2020, 2150), y -> if_then_else(apply_continuous(SLR_curve, y) >= 0.5, y, null_of('integer'))) as year_50cm_exceeded,
         map(range(2020, 2150), y -> if_then_else(apply_continuous(SLR_curve, y) >= 1.0, y, null_of('integer'))) as year_100cm_exceeded,
         map(range(2020, 2150), y -> if_then_else(apply_continuous(SLR_curve, y) >= 1.5, y, null_of('integer'))) as year_150cm_exceeded
      })
 ->
select({
        Scenario,
        Percentile,
        min(year_50cm_exceeded) as SLR_50cm,
        min(year_100cm_exceeded) as SLR_100cm,
        min(year_150cm_exceeded) as SLR_150cm,
      })
 ->
select({
        *,
        if_then_else(is_null(SLR_50cm), 'N/A', str(SLR_50cm)) as SLR_50cm,
        if_then_else(is_null(SLR_100cm), 'N/A', str(SLR_100cm)) as SLR_100cm,
        if_then_else(is_null(SLR_150cm), 'N/A', str(SLR_150cm)) as SLR_150cm,
      })
 ->
sort([SLR_50cm])
 ->
save('year-SLR-exceeded', format: 'csv')
